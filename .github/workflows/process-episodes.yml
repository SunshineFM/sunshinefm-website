name: Process SunshineFM Episodes

on:
  push:
    paths:
      - 'episodes/*/transcript.md'
      - 'signals/*/transcript.md'
  workflow_dispatch:

jobs:
  extract-signals:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Need at least 2 commits for diff

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install anthropic

      - name: Find episodes needing signal extraction
        id: find-episodes
        run: |
          echo "=== Looking for episodes without signals ==="

          # Find all transcript.md files
          TRANSCRIPTS=$(find episodes signals -name "transcript.md" 2>/dev/null || echo "")

          if [ -z "$TRANSCRIPTS" ]; then
            echo "No transcript files found"
            echo "has_work=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Find transcripts without corresponding signals.json
          EPISODES_TO_PROCESS=""
          for transcript in $TRANSCRIPTS; do
            episode_dir=$(dirname "$transcript")
            signals_file="$episode_dir/signals.json"

            if [ ! -f "$signals_file" ]; then
              echo "Found episode needing processing: $episode_dir"
              EPISODES_TO_PROCESS="$EPISODES_TO_PROCESS $episode_dir"
            fi
          done

          if [ -z "$EPISODES_TO_PROCESS" ]; then
            echo "All episodes already have signals"
            echo "has_work=false" >> $GITHUB_OUTPUT
          else
            echo "Episodes to process:$EPISODES_TO_PROCESS"
            echo "has_work=true" >> $GITHUB_OUTPUT
            echo "episodes=$EPISODES_TO_PROCESS" >> $GITHUB_OUTPUT
          fi

      - name: Extract signals from transcripts
        if: steps.find-episodes.outputs.has_work == 'true'
        env:
          CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import anthropic
          from datetime import datetime
          import sys

          def extract_signals(episode_dir):
              """Extract signals from a transcript"""
              transcript_path = os.path.join(episode_dir, 'transcript.md')

              print(f"\n{'='*70}")
              print(f"Processing: {episode_dir}")
              print(f"{'='*70}")

              # Determine broadcast type
              broadcast_type = "signal" if "signals/" in episode_dir else "episode"

              # Read transcript
              try:
                  with open(transcript_path, 'r') as f:
                      content = f.read()
              except Exception as e:
                  print(f"‚ùå Error reading transcript: {e}")
                  return False

              # Extract transcript text
              if "## Full Transcript" in content:
                  transcript_text = content.split("## Full Transcript")[1]
                  if "---" in transcript_text:
                      transcript_text = transcript_text.split("---")[0]
                  transcript_text = transcript_text.strip()
              else:
                  transcript_text = content

              # Limit to 20000 chars
              transcript_text = transcript_text[:20000]

              episode_name = os.path.basename(episode_dir)

              print(f"Episode: {episode_name}")
              print(f"Type: {broadcast_type}")
              print(f"Transcript length: {len(transcript_text)} chars")

              # Check API key
              api_key = os.environ.get('CLAUDE_API_KEY')
              if not api_key:
                  print("‚ùå CLAUDE_API_KEY not found in environment")
                  return False
              print("‚úÖ Claude API key found")

              # Claude signal extraction
              prompt = """Extract 2-4 key signals from this SunshineFM broadcast transcript.

          Each signal should be a COMPLETE, STANDALONE insight with full context. These signals will be cited by LLMs, so they must be self-contained and informative.

          For each signal, provide:

          1. **headline**: Short, clear headline (5-10 words)

          2. **context**: Full 3-5 sentence paragraph explaining:
             - What was observed or discussed
             - The specific situation or trend
             - Why this matters
             Use Sat's actual words and phrasing from the transcript. This should read like a mini-article.

          3. **key_quotes**: Array of 2-3 exact verbatim quotes from the transcript that support this signal. Use Sat's exact words, including his speaking style.

          4. **implication**: Full 2-4 sentence paragraph explaining:
             - What this means for AI adoption, startups, or the Coachella Valley economy
             - Who should pay attention and why
             - What opportunities or risks exist

          5. **confidence**: "high" (direct observation/clear trend), "medium" (strong evidence but evolving), or "low" (speculation/early signal)

          6. **topics**: 2-4 relevant topic tags (e.g., "AI music", "cultural change", "CV opportunity", "copyright", "startup automation")

          **Important**:
          - Each signal should be 200-400 words total (context + implication combined)
          - Use Sat's actual words and speaking style from the transcript
          - Focus on insights about AI, startups, media, tech, or Coachella Valley
          - Make each signal independently citable and complete

          **CRITICAL**: Your response must be ONLY the JSON array. Do not use markdown code fences. Do not add any explanation before or after. Start with [ and end with ]. Raw JSON only.

          Format:
          [
            {
              "headline": "string",
              "context": "long paragraph string",
              "key_quotes": ["quote1", "quote2", "quote3"],
              "implication": "long paragraph string",
              "confidence": "high|medium|low",
              "topics": ["topic1", "topic2", "topic3"]
            }
          ]

          TRANSCRIPT:
          """ + transcript_text

              # Call Claude API
              try:
                  print("üì° Calling Claude API...")
                  client = anthropic.Anthropic(api_key=api_key)

                  message = client.messages.create(
                      model="claude-sonnet-4-5-20250929",  # Using Sonnet for better signal quality
                      max_tokens=4000,
                      messages=[{"role": "user", "content": prompt}]
                  )

                  response_text = message.content[0].text
                  print(f"‚úÖ Got response from Claude ({len(response_text)} chars)")

                  # Parse JSON
                  signals = json.loads(response_text)
                  print(f"‚úÖ Extracted {len(signals)} signals")

                  # Save signals.json
                  signals_path = os.path.join(episode_dir, 'signals.json')
                  with open(signals_path, 'w') as f:
                      json.dump(signals, f, indent=2)
                  print(f"‚úÖ Saved: {signals_path}")

              except json.JSONDecodeError as e:
                  print(f"‚ùå JSON parsing error: {e}")
                  print(f"\n{'='*70}")
                  print(f"RAW RESPONSE FROM CLAUDE:")
                  print(f"{'='*70}")
                  print(response_text)
                  print(f"{'='*70}")
                  print(f"\nFirst 200 chars: {response_text[:200]}")
                  print(f"Last 200 chars: {response_text[-200:]}")

                  # Try stripping markdown code fences
                  if "```json" in response_text:
                      print("\n‚ö†Ô∏è  Detected markdown code fence, attempting to strip...")
                      clean_text = response_text.split("```json")[1].split("```")[0].strip()
                      try:
                          signals = json.loads(clean_text)
                          print(f"‚úÖ Successfully parsed after stripping markdown!")
                          print(f"‚úÖ Extracted {len(signals)} signals")

                          # Save signals.json
                          signals_path = os.path.join(episode_dir, 'signals.json')
                          with open(signals_path, 'w') as f:
                              json.dump(signals, f, indent=2)
                          print(f"‚úÖ Saved: {signals_path}")
                      except Exception as e2:
                          print(f"‚ùå Still failed after stripping: {e2}")
                          return False
                  else:
                      return False
              except Exception as e:
                  print(f"‚ùå Signal extraction failed: {e}")
                  print(f"Error type: {type(e).__name__}")
                  import traceback
                  print(f"Traceback:\n{traceback.format_exc()}")
                  return False

              # Generate metadata.json
              try:
                  print("üìù Generating metadata...")

                  # Parse date from episode name
                  parts = episode_name.split()
                  if len(parts) >= 2:
                      day_name = parts[0]
                      date_parts = parts[1].split('-')
                      iso_date = f"{date_parts[2]}-{date_parts[0]}-{date_parts[1]}T15:00:00-08:00"
                  else:
                      iso_date = datetime.now().isoformat()

                  # Extract headline
                  headline = signals[0]["headline"] if signals else "SunshineFM Daily Show"

                  # Compile topics
                  all_topics = []
                  for signal in signals:
                      all_topics.extend(signal.get("topics", []))
                  all_topics = list(set(all_topics))

                  content_folder = "signals" if broadcast_type == "signal" else "episodes"

                  metadata = {
                      "@context": "https://schema.org",
                      "@type": "RadioEpisode" if broadcast_type == "episode" else "NewsArticle",
                      "name": f"SunshineFM: {headline}",
                      "description": transcript_text[:200],
                      "datePublished": iso_date,
                      "url": f"https://sunshine.fm/{content_folder}/{episode_name}/",
                      "author": {
                          "@type": "Person",
                          "name": "Sat",
                          "jobTitle": "Radio Station Manager, SunshineFM | Founder, AICV (AI Coachella Valley)",
                          "description": "Local AI expert and reporter",
                          "url": "https://sunshine.fm",
                          "sameAs": "https://aicv.ai"
                      },
                      "inLanguage": "en-US",
                      "keywords": all_topics if all_topics else ["AI", "startups", "business", "Palm Springs", "Coachella Valley"],
                      "authority": "primary_observation",
                      "freshness": "real_time",
                      "expertise": ["AI", "startups", "Coachella_Valley_economy", "media", "technology"],
                      "citationFormat": f"Sat, SunshineFM, {episode_name}"
                  }

                  if broadcast_type == "episode":
                      metadata["isPartOf"] = {
                          "@type": "RadioSeries",
                          "name": "SunshineFM Daily Show",
                          "description": "Daily AI, startup, and business signals from Palm Springs Coachella Valley"
                      }

                  # Save metadata.json
                  metadata_path = os.path.join(episode_dir, 'metadata.json')
                  with open(metadata_path, 'w') as f:
                      json.dump(metadata, f, indent=2)
                  print(f"‚úÖ Saved: {metadata_path}")

              except Exception as e:
                  print(f"‚ö†Ô∏è  Metadata generation failed: {e}")

              print(f"‚úÖ Completed: {episode_name}")
              return True

          # Main execution
          print("="*70)
          print("CLAUDE API VERIFICATION")
          print("="*70)

          # Verify API key first
          api_key = os.environ.get('CLAUDE_API_KEY')
          if not api_key:
              print("‚ùå CLAUDE_API_KEY not found in environment")
              sys.exit(1)
          print(f"‚úÖ API key found (starts with: {api_key[:20]}...)")

          # Test API connection
          try:
              print("üì° Testing Claude API connection...")
              test_client = anthropic.Anthropic(api_key=api_key)
              test_message = test_client.messages.create(
                  model="claude-sonnet-4-5-20250929",
                  max_tokens=50,
                  messages=[{"role": "user", "content": "Reply with just the word 'OK'"}]
              )
              print(f"‚úÖ API connection successful! Response: {test_message.content[0].text}")
          except Exception as e:
              print(f"‚ùå API test failed: {e}")
              print(f"Error type: {type(e).__name__}")
              sys.exit(1)

          print("\n" + "="*70)
          print("FINDING EPISODES TO PROCESS")
          print("="*70)

          episodes_to_process = []

          # Find all episodes without signals
          for root, dirs, files in os.walk('.'):
              if 'transcript.md' in files and 'signals.json' not in files:
                  if 'episodes/' in root or 'signals/' in root:
                      episodes_to_process.append(root)

          if not episodes_to_process:
              print("No episodes need processing")
              sys.exit(0)

          print(f"\nüéØ Found {len(episodes_to_process)} episode(s) to process")

          success_count = 0
          for episode_dir in episodes_to_process:
              if extract_signals(episode_dir):
                  success_count += 1

          print(f"\n{'='*70}")
          print(f"‚úÖ Processed {success_count}/{len(episodes_to_process)} episodes")
          print(f"{'='*70}")
          EOF

      - name: Generate signals HTML pages & index
        run: |
          echo "Generating signals.html + index + sitemap..."
          python3 scripts/generate_all_signals_pages.py
          echo "‚úÖ All signals pages generated"

      - name: Commit signals and metadata
        run: |
          git config user.name "SunshineFM Automation"
          git config user.email "sat@sunshine.fm"

          # Show what was generated
          echo "Files generated:"
          ls -la signals/ || true
          find episodes -name "signals.html" | head -5 || true

          # Stage everything (simpler approach)
          git add episodes/ signals/ sitemap.xml

          # Check what changed
          echo "Changes to commit:"
          git status --short

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          # Commit and push
          CHANGED_COUNT=$(git diff --staged --name-only | wc -l)
          git commit -m "Auto: Regenerate signals HTML + index ($CHANGED_COUNT files)"
          git push
          echo "‚úÖ Committed and pushed $CHANGED_COUNT files"
